---
title: "R Notebook"
output: html_notebook
---

# Todo

- Check for missing data across columns (including response), as well as alternate ways data might be NA
- Perform EDA to sanity check values, check types, etc.

# Changes to data

- Filtered data such that `api_temp = oxymorphone`
- Dropped `country`, `api_temp`, and 'form_temp`
- Removed observations with missing `ppm` (14)
- Dropped unused levels
- Dropped observations where `state == 'USA'`

```{r include=FALSE}
knitr::opts_chunk$set('echo'=FALSE)

library(dplyr)
library(ggplot2)
library(lubridate)

load('streetrx.rdata')
streetrx <- streetrx %>% filter(api_temp == 'oxymorphone')
streetrx <- streetrx %>% filter(!is.na(ppm))
streetrx$log_ppm <- log(streetrx$ppm)

# Drop unusued levels:
streetrx <- droplevels(streetrx)

# 1 country (USA), 1 drug (oxymorphone), and always as a pill/tablet (except once)
drop_cols <- c('country', 'api_temp', 'form_temp')
streetrx <- streetrx %>% select(-any_of(drop_cols))
```

```{r}
set.seed(42)
slice_sample(streetrx, n=20)
```

# Initial observations (brain dump)

- At least one of the response variables (`ppm`) is missing - we'll need to remove these observations
- `yq_pdate` and `price_date` contain redundant information - possibly best to break these 2 columns apart into quarter, day, month, and year columns. Likely day won't be important after accounting for month and year - check a random set of same month/year pairings and see if there's a trend?
- `city` frequently missing
- `state` contains 59 levels: will need to look into this
- `country` column not needed (only working with USA)
- `source` looks frequently missing and it's not clear how many levels there are: look into
- `api_temp`: Not needed (we're only working with oxymorphone)
- `mgstr`: Will need to validate this - shouldn't be negative or 0 (check histogram)
- `bulk_purchase`: Remove the text
- `primary_reason`: Looks missing frequently

# Checking Columns One-byone

**Keep, log transform**

Our response. First, let's take a look at which fields have missing values and how frequently they occur. First, the response variable

```{r}
sum(is.na(streetrx$ppm))
```

We'll need to drop these, because a missing response provides no assistance to our model:

```{r}
streetrx <- streetrx %>% filter(!is.na(ppm))
```

Any other weird values?

```{r}
ggplot(streetrx, aes(x=log(ppm))) + geom_histogram(bins = 50)
```

There are some very extreme values here:

```{r}
streetrx %>% filter(ppm > 10)
```

I'm certainly not an expert on drugs. But based on the previous prices, these are likely erroneously recorded, or not serious entries. Keep an eye on and consider dropping - the corresponding points could be influential

## yq_pdate

This variable contains the year of sale as well as the quarter (1-4). We already have the year extracted, let's get the quarter. First are all the dates as expected?

```{r}
summary(streetrx$yq_pdate)
```

Well, we certainly have a year that is likely wrong (1969), but at least all of these values contain 5 digits

```{r}
# To find the final number (quarter), simply mod 10
streetrx$quarter <- streetrx$yq_pdate %% 10
```

Let's take a look at the density:

```{r}
small_ppm <- streetrx %>% filter(ppm < 10)
ggplot(small_ppm, aes(x=ppm, color=as.factor(quarter))) + geom_density(bw=0.8)
```

## Price Date

Firstly, change this to a date and extract its component parts:

```{r}
streetrx$price_date <- as.Date(streetrx$price_date, format= '%m/%d/%y')
streetrx <- streetrx %>% mutate(year = lubridate::year(price_date),
                                month = as.factor(lubridate::month(price_date)),
                                day = lubridate::day(price_date))
```

I wouldn't think we should include year, as presumably we're going to be using this for future data, for which we'll have a new year. What about month? We'll use median because there are some extreme ppm values

```{r}
streetrx %>% group_by(month) %>% summarise(med_ppm = median(ppm))
```

An F test shows that we cannot reject the possibility that this variance is simply due to chance:

```{r}
aov(ppm ~ month, data = streetrx) %>% summary()
```


## City

First, check for standard missing values

```{r}
sum(is.na(streetrx$city))
```

Some issues:

- Missing city is the most common (1413 times)
- Nonstandard casing (sometimes lowercase)
- Numbers occasionally appear
- State occasionally included in city

Honestly this column is a bit of a mess, with many values appearing a very minimal number of times:

```{r}
table(streetrx$city, exclude = '') %>% hist()
```

Don't include for now

## State

Missing values?

```{r}
sum(is.na(streetrx$state))
```

No - let's check a table then:

```{r}
table(streetrx$state)
```

Ok, we have 52 levels but we *should* have 50. D.C. is on there which is fine. The other incorrect value is a state of "USA". We'll probably have to drop these, or we could take a look at their cities and manually enter the state

```{r}
streetrx %>% filter(state == 'USA')
```

City is missing, region is missing - we don't have any location information on these observations: drop

```{r}
streetrx <- streetrx %>% filter(state != 'USA') %>% droplevels()
```

## USA region

Recode missing as "missing"

```{r}
table(streetrx$USA_region)
```

Let's visualize the estimated density for these 4 groups. For visualization purposes, we omit ppm greater than 10 (which is largely the entire data anyawy)

```{r}
small_ppm <- streetrx %>% filter(ppm < 10)
ggplot(small_ppm, aes(x=ppm, color=USA_region)) + geom_density(bw=0.5)
```

They all look pretty similar actually *except* for South, which seems to be higher priced than the other on average.

## Source

Missing (empty), internet (interet pharmacy, url, google), heard it, personal, other

Only 18 levels - let's check the table output:

```{r}
table(streetrx$source)
```

Let's recode some of these:

```{r}
# Recode "missing" values to NA
levels(streetrx$source)[levels(streetrx$source) == ''] <- 'Missing'
streetrx$source <- recode_factor(streetrx$source, 'w'='Missing',
                                 'Idk'='Missing')

streetrx$source <- recode_factor(streetrx$source,
                                 'Www.drugforum.com' = 'Internet',
                                 'Google.com' = 'Internet',
                                 'opiophile' = 'Internet',
                                 'silkroad' = 'Internet',
                                 'bluelight' = 'Internet',
                            'http://www.kentucky.com/2011/08/05/1835152/union-college-presidents-wife.html' = 'Internet')

for(website in c('bluelight', 'opiophile', 'silkroad')) {
    levels(streetrx$source)[grepl(website, levels(streetrx$source), fixed=TRUE)] <- website    
}
```

There's still work to be done on this one. Ultimately about 1/3 of these values are missing - we might not be able to save it from that.

## mgstr

Dosage strength in mg of the units purchased. Let's check a summary:

```{r}
summary(streetrx$mgstr)
```

Not actually a continuous variable:

```{r}
table(streetrx$mgstr)
```

```{r}
mgstr_grouped <- streetrx %>% group_by(mgstr) %>% summarise(avg=mean(log_ppm))
ggplot(mgstr_grouped, aes(x=mgstr, y=avg)) + geom_line()
```

```{r}
ggplot(streetrx, aes(x=log_ppm)) + geom_histogram(bins = 50)
```


## bulk_purchase

```{r}
table(streetrx$bulk_purchase)
```

```{r}
ggplot(streetrx, aes(x=bulk_purchase, y=log_ppm)) + geom_boxplot()
```

It looks like bulk purchases are generally associated with cheaper prices. But are these differences in means significant? We may apply the t-test because the CLT kicks in for these large groups

```{r}
t.test(log_ppm ~ bulk_purchase, data = streetrx)
```

So the two bulk purchase values look to have significantly different log ppm prices

## Primary Reason

```{r}
table(streetrx$Primary_Reason)
```

First of all, it seems like "reporter did not answer the question" and missing should belong to the same category. Let's see:

```{r}
t.test(streetrx[streetrx$Primary_Reason == '',]$log_ppm,
       streetrx[streetrx$Primary_Reason == '0 Reporter did not answer the question',]$log_ppm)
```

